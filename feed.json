{
    "version": "https://jsonfeed.org/version/1",
    "title": "dunghv.github.io",
    "description": "",
    "home_page_url": "https://dunghvcs.github.io",
    "feed_url": "https://dunghvcs.github.io/feed.json",
    "user_comment": "",
    "icon": "https://dunghvcs.github.io/media/website/CVISLab-removebg-preview.png",
    "author": {
        "name": "Van-Dung Hoang"
    },
    "items": [
        {
            "id": "https://dunghvcs.github.io/project2019-virtual-tour.html",
            "url": "https://dunghvcs.github.io/project2019-virtual-tour.html",
            "title": "Project2019-Virtual tour",
            "summary": "Virtual tour development of famous places in Quang Binh using panoramic stitching, 3D interaction and modern web technology, which funded by the QB People's Committee. Collection of Tourist Site Information The research team conducted field surveys at 32 scenic and tourist destinations across Qu·∫£ng B√¨nh in accordance with the proposed&hellip;",
            "content_html": "<div class=\"CjVfdc CJIdie\"><span class=\"C9DxTc \">Virtual tour development of famous places in Quang Binh using panoramic stitching, 3D interaction and modern web technology, which funded by the QB People's Committee.</span></div>\n<div>\n<p data-start=\"0\" data-end=\"476\"><strong data-start=\"0\" data-end=\"49\">Collection of Tourist Site Information</strong><br data-start=\"49\" data-end=\"52\">The research team conducted field surveys at 32 scenic and tourist destinations across <span class=\"hover:entity-accent entity-underline inline cursor-pointer align-baseline\"><span class=\"whitespace-normal\">Qu·∫£ng B√¨nh</span></span> in accordance with the proposed research objectives and collected relevant data. The collected information includes site names, tourism types, addresses, GPS locations, and basic descriptive characteristics, which serve as reference data for the subsequent process of image and dataset collection.</p>\n<p data-start=\"478\" data-end=\"974\"><strong data-start=\"478\" data-end=\"512\">Data Collection¬†</strong><br data-start=\"512\" data-end=\"515\">The research team carried out surveys and gathered information and written materials for 32 scenic and tourist destinations in Qu·∫£ng B√¨nh province. The collected data include descriptive information about tourist sites, associated service systems, tourism promotional articles, the number of photographs taken at each site for panoramic image construction, and images used for album creation. All data were processed to ensure adequate quantity and quality.</p>\n<p data-start=\"976\" data-end=\"1247\" data-is-last-node=\"\" data-is-only-node=\"\"><strong data-start=\"976\" data-end=\"1017\">Software System Design</strong><br data-start=\"1017\" data-end=\"1020\">The software system is designed to meet management requirements, provide information about tourist destinations, support virtual tourism experiences, display photo albums, present related services, and enable video integration.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/16/Detai_tinh2019_1.png\" alt=\"\" width=\"842\" height=\"760\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_1-xs.png 640w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_1-sm.png 768w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_1-md.png 1024w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_1-lg.png 1366w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_1-xl.png 1600w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_1-2xl.png 1920w\"></figure>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/16/Detai_tinh2019_2.png\" alt=\"\" width=\"1168\" height=\"774\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_2-xs.png 640w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_2-sm.png 768w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_2-md.png 1024w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_2-lg.png 1366w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_2-xl.png 1600w ,https://dunghvcs.github.io/media/posts/16/responsive/Detai_tinh2019_2-2xl.png 1920w\"></figure>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/16/IMG-0366.JPG\" alt=\"\" width=\"1366\" height=\"768\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/16/responsive/IMG-0366-xs.JPG 640w ,https://dunghvcs.github.io/media/posts/16/responsive/IMG-0366-sm.JPG 768w ,https://dunghvcs.github.io/media/posts/16/responsive/IMG-0366-md.JPG 1024w ,https://dunghvcs.github.io/media/posts/16/responsive/IMG-0366-lg.JPG 1366w ,https://dunghvcs.github.io/media/posts/16/responsive/IMG-0366-xl.JPG 1600w ,https://dunghvcs.github.io/media/posts/16/responsive/IMG-0366-2xl.JPG 1920w\"></figure>\n</div>\n<div>¬†</div>\n<h3 id=\"h.fklu04cvv6wh_l\" class=\"zfr3Q OmQG5e CDt4Ke \" dir=\"ltr\"></h3>",
            "image": "https://dunghvcs.github.io/media/posts/16/Tinh2-2.png",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Researches"
            ],
            "date_published": "2026-02-14T10:15:14+07:00",
            "date_modified": "2026-02-14T10:15:14+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/project-2015-funded-by-the-qb-peoples-committee.html",
            "url": "https://dunghvcs.github.io/project-2015-funded-by-the-qb-peoples-committee.html",
            "title": "Project 2015- Funded by the QB People&#x27;s Committee",
            "summary": "Proposal a computer vision- based surveillance system applied to secured to office building in Quang Binh province. General Objectives: To investigate and analyze the current state of technology applications in ensuring security for socio-economic development. To propose a real-time security recognition, tracking, and alert system model for office buildings. Specific&hellip;",
            "content_html": "<div class=\"CjVfdc\"><span class=\"C9DxTc \">Proposal a computer vision- based surveillance system applied to secured to office building in Quang Binh province.</span></div>\n<div>\n<p data-start=\"0\" data-end=\"25\"><strong data-start=\"0\" data-end=\"23\">General Objectives:</strong></p>\n<ul data-start=\"26\" data-end=\"258\">\n<li data-start=\"26\" data-end=\"154\">\n<p data-start=\"28\" data-end=\"154\">To investigate and analyze the current state of technology applications in ensuring security for socio-economic development.</p>\n</li>\n<li data-start=\"155\" data-end=\"258\">\n<p data-start=\"157\" data-end=\"258\">To propose a real-time security recognition, tracking, and alert system model for office buildings.</p>\n</li>\n</ul>\n<p data-start=\"260\" data-end=\"286\"><strong data-start=\"260\" data-end=\"284\">Specific Objectives:</strong></p>\n<ul data-start=\"287\" data-end=\"888\">\n<li data-start=\"287\" data-end=\"334\">\n<p data-start=\"289\" data-end=\"334\">To retrieve data from surveillance cameras.</p>\n</li>\n<li data-start=\"335\" data-end=\"699\">\n<p data-start=\"337\" data-end=\"372\">Recognition and tracking methods:</p>\n<ul data-start=\"375\" data-end=\"699\">\n<li data-start=\"375\" data-end=\"424\">\n<p data-start=\"377\" data-end=\"424\">Propose image feature representation methods.</p>\n</li>\n<li data-start=\"427\" data-end=\"491\">\n<p data-start=\"429\" data-end=\"491\">P</p>\n</li>\n<li data-start=\"427\" data-end=\"491\">\n<p data-start=\"429\" data-end=\"491\">ropose models for target objects to be recognized (humans).</p>\n</li>\n<li data-start=\"494\" data-end=\"568\">\n<p data-start=\"496\" data-end=\"568\">Propose artificial intelligence frameworks for training object models.</p>\n</li>\n<li data-start=\"571\" data-end=\"639\">\n<p data-start=\"573\" data-end=\"639\">Propose and develop systems for object recognition and tracking.</p>\n</li>\n<li data-start=\"642\" data-end=\"699\">\n<p data-start=\"644\" data-end=\"699\">Build datasets for training, evaluation, and testing.</p>\n</li>\n</ul>\n</li>\n<li data-start=\"701\" data-end=\"839\">\n<p data-start=\"703\" data-end=\"722\"><strong data-start=\"703\" data-end=\"720\">Alert System:</strong></p>\n<ul data-start=\"725\" data-end=\"839\">\n<li data-start=\"725\" data-end=\"781\">\n<p data-start=\"727\" data-end=\"781\">Analyze and develop semantic representation systems.</p>\n</li>\n<li data-start=\"784\" data-end=\"839\">\n<p data-start=\"786\" data-end=\"839\">Build decision-support systems for security alerts.</p>\n</li>\n</ul>\n</li>\n<li data-start=\"841\" data-end=\"888\">\n<p data-start=\"843\" data-end=\"888\">Conduct system experiments and evaluations.</p>\n</li>\n</ul>\n<p data-start=\"890\" data-end=\"919\"><strong data-start=\"890\" data-end=\"917\">Research Subjects:</strong></p>\n<ul data-start=\"920\" data-end=\"1131\" data-is-last-node=\"\" data-is-only-node=\"\">\n<li data-start=\"920\" data-end=\"984\">\n<p data-start=\"922\" data-end=\"984\">Suspicious individuals illegally entering office facilities.</p>\n</li>\n<li data-start=\"985\" data-end=\"1059\">\n<p data-start=\"987\" data-end=\"1059\">Image processing, feature representation, and artificial intelligence.</p>\n</li>\n<li data-start=\"1060\" data-end=\"1131\" data-is-last-node=\"\">\n<p data-start=\"1062\" data-end=\"1131\" data-is-last-node=\"\">Expert systems and contextual reasoning applied to security alerting.</p>\n</li>\n</ul>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/15/IMG_2418.jpg\" alt=\"\" width=\"2542\" height=\"974\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/15/responsive/IMG_2418-xs.jpg 640w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2418-sm.jpg 768w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2418-md.jpg 1024w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2418-lg.jpg 1366w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2418-xl.jpg 1600w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2418-2xl.jpg 1920w\"></figure>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/15/IMG_2371.jpg\" alt=\"\" width=\"2592\" height=\"1339\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/15/responsive/IMG_2371-xs.jpg 640w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2371-sm.jpg 768w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2371-md.jpg 1024w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2371-lg.jpg 1366w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2371-xl.jpg 1600w ,https://dunghvcs.github.io/media/posts/15/responsive/IMG_2371-2xl.jpg 1920w\"></figure>\n</div>\n<h3 id=\"h.o4ldey3lih43_l\" class=\"zfr3Q OmQG5e CDt4Ke \" dir=\"ltr\"></h3>",
            "image": "https://dunghvcs.github.io/media/posts/15/IMG_2346.jpg",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Researches"
            ],
            "date_published": "2026-02-14T10:02:05+07:00",
            "date_modified": "2026-02-14T10:04:34+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/ulsan2023-visiting-research.html",
            "url": "https://dunghvcs.github.io/ulsan2023-visiting-research.html",
            "title": "Ulsan2023 - Visiting research",
            "summary": "Visiting research at¬†Ulsan ISLab¬†(Intelligent Systems Laboratory) in 2023, led by Professor Kang-Hyun Jo, focuses heavily on advanced computer vision applications, including generative models, 3D reconstruction, autonomous driving systems, and human motion/gait analysis. Key areas often involve 3D facial landmark detection and intelligent video systems.",
            "content_html": "<p>Visiting research at¬†<span data-wiz-uids=\"EvebVb_b\" data-processed=\"true\"><a jsuid=\"EvebVb_b\" class=\"GI370e\" data-ved=\"2ahUKEwiKr_u4_deSAxXbcPUHHZWdB98QgK4QegQIARAB\" data-hveid=\"CAEQAQ\" data-processed=\"true\" href=\"https://www.google.com/search?q=Ulsan+ISLab&amp;sca_esv=4e4f828e227a99b7&amp;rlz=1C1YTUH_viVN1067VN1067&amp;biw=1920&amp;bih=911&amp;sxsrf=ANbL-n5v7v0nqWMcvqe6sEudzB43FR2XTg%3A1771037561148&amp;ei=eeOPaZrgCI7i2roPsLDa-QI&amp;ved=2ahUKEwiKr_u4_deSAxXbcPUHHZWdB98QgK4QegQIARAB&amp;uact=5&amp;oq=visiting+research+at+Ulsan+Islab+2023++on+Compuiter+vision+topic&amp;gs_lp=Egxnd3Mtd2l6LXNlcnAiQHZpc2l0aW5nIHJlc2VhcmNoIGF0IFVsc2FuIElzbGFiIDIwMjMgIG9uIENvbXB1aXRlciB2aXNpb24gdG9waWNIjrgBUNwHWMW2AXALeACQAQSYAd8BoAHfOqoBBjIuNTQuMbgBA8gBAPgBAZgCOaACvzDCAgoQABiwAxjWBBhHwgIFECEYoAHCAgYQABgWGB7CAgsQABiABBiGAxiKBcICBRAAGO8FwgIIEAAYgAQYogTCAgQQIxgnwgIIEAAYFhgKGB7CAgsQABiABBiRAhiKBcICBRAAGIAEwgIHECEYoAEYCsICBBAhGBXCAgQQIRgKwgIGECEYFRgKmAMAiAYBkAYIkgcFMTIuNDWgB_X7AbIHBDEuNDW4B5AwwgcJMy4zNS4xOC4xyAebAYAIAA&amp;sclient=gws-wiz-serp\">Ulsan ISLab</a></span>¬†(Intelligent Systems Laboratory) in 2023, led by Professor Kang-Hyun Jo, focuses heavily on advanced computer vision applications, including generative models, 3D reconstruction, autonomous driving systems, and human motion/gait analysis. Key areas often involve 3D facial landmark detection and intelligent video systems.<span class=\"uJ19be notranslate\" data-wiz-uids=\"EvebVb_d,EvebVb_e\" data-processed=\"true\"><span class=\"vKEkVd\" data-animation-atomic=\"\" data-wiz-attrbind=\"class=EvebVb_c/TKHnVd\" data-processed=\"true\"><span aria-hidden=\"true\" data-processed=\"true\">¬†</span></span></span></p>\n<ul class=\"KsbFXc U6u95\" data-processed=\"true\">\n<li data-hveid=\"CAMQAA\" data-processed=\"true\"><span class=\"T286Pc\" data-sfc-cp=\"\" data-processed=\"true\"><strong class=\"Yjhzub\" data-processed=\"true\">3D Vision and Reconstruction:</strong>¬†Techniques for scene modeling and 3D pose estimation.</span></li>\n<li data-hveid=\"CAMQAQ\" data-processed=\"true\"><span class=\"T286Pc\" data-sfc-cp=\"\" data-processed=\"true\"><strong class=\"Yjhzub\" data-processed=\"true\">Intelligent Transportation/Driving:</strong>¬†Computer vision algorithms for autonomous vehicles and event detection.</span></li>\n<li data-hveid=\"CAMQAg\" data-processed=\"true\"><span class=\"T286Pc\" data-sfc-cp=\"\" data-processed=\"true\"><strong class=\"Yjhzub\" data-processed=\"true\">Computer Vision &amp; AI:</strong>¬†Deep learning techniques for image understanding, object detection, and tracking.</span></li>\n<li data-hveid=\"CAMQAw\" data-processed=\"true\"><span class=\"T286Pc\" data-sfc-cp=\"\" data-processed=\"true\"><strong class=\"Yjhzub\" data-processed=\"true\">Human Motion Analysis:</strong>¬†Analyzing gait and human movement for recognition task</span></li>\n</ul>\n<p>¬†</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/14/Ulsan20231_2.jpg\" alt=\"\" width=\"2560\" height=\"1152\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_2-xs.jpg 640w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_2-sm.jpg 768w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_2-md.jpg 1024w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_2-lg.jpg 1366w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_2-xl.jpg 1600w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_2-2xl.jpg 1920w\"></figure>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/14/Ulsan20231_1-2.JPG\" alt=\"\" width=\"4032\" height=\"3024\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_1-2-xs.JPG 640w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_1-2-sm.JPG 768w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_1-2-md.JPG 1024w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_1-2-lg.JPG 1366w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_1-2-xl.JPG 1600w ,https://dunghvcs.github.io/media/posts/14/responsive/Ulsan20231_1-2-2xl.JPG 1920w\"></figure>",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Seminars",
                   "News"
            ],
            "date_published": "2026-02-14T09:56:24+07:00",
            "date_modified": "2026-02-14T09:56:24+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/aciids2023.html",
            "url": "https://dunghvcs.github.io/aciids2023.html",
            "title": "ACIIDS2023",
            "summary": "ACIIDS 2023 is an international scientific conference for research in the field of intelligent information and database systems, held from 24 to 26 of July 2023 in Phuket, Thailand. The conference aims to provide an internationally respected forum for scientific research in intelligent information and database systems technologies and applications.",
            "content_html": "<p>ACIIDS 2023 is an international scientific conference for research in the field of intelligent information and database systems, held from 24 to 26 of July 2023 in Phuket, Thailand.</p>\n<p>The conference aims to provide an internationally respected forum for scientific research in intelligent information and database systems technologies and applications. ACIIDS conference is ranked category B in the prestigious¬†<a href=\"http://portal.core.edu.au/conf-ranks/2188/\">CORE ranking</a>.</p>\n<p>The conference is hosted by King Mongkut‚Äôs Institute of Technology Ladkrabang, Thailand, and jointly organized by Wroc≈Çaw University of Science and Technology, Poland, in cooperation with IEEE SMC Technical Committee on Computational Collective Intelligence, European Research Center for Information Systems (ERCIS), University of Newcastle (Australia), Yeungnam University (Korea), International University - Vietnam National University HCMC (Vietnam), Leiden University (The Netherlands), Universiti Teknologi Malaysia (Malaysia), Ton Duc Thang University (Vietnam), BINUS University (Indonesia), and Vietnam National University, Hanoi (Vietnam).</p>\n<p>The proceedings of ACIIDS 2023 will be published by Springer.</p>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">CVISLab member presents a contribution on \"Combination of Deep Learning and Ambiguity Rejection for Improving Image-Based Disease Diagnosis\".</span></p>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">A leader of CVISLab achieves outstanding contribution award for 15 Years of ACIIDS conference</span></p>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">Over all, everything is well organized.</span></p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/13/ACIIDS2023_2.jpg\" alt=\"\" width=\"1280\" height=\"1818\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_2-xs.jpg 640w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_2-sm.jpg 768w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_2-md.jpg 1024w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_2-lg.jpg 1366w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_2-xl.jpg 1600w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_2-2xl.jpg 1920w\"></figure>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/13/ACIIDS2023_1.jpg\" alt=\"\" width=\"959\" height=\"539\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_1-xs.jpg 640w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_1-sm.jpg 768w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_1-md.jpg 1024w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_1-lg.jpg 1366w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_1-xl.jpg 1600w ,https://dunghvcs.github.io/media/posts/13/responsive/ACIIDS2023_1-2xl.jpg 1920w\"></figure>",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Conference Events"
            ],
            "date_published": "2026-02-14T09:52:08+07:00",
            "date_modified": "2026-02-14T09:52:08+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/icsse-2023.html",
            "url": "https://dunghvcs.github.io/icsse-2023.html",
            "title": "ICSSE 2023",
            "summary": "ICSSE 2023:¬†The Annual International Conference on System Science and Engineering, well known as ICSSE, is a reputational conference on System Science and Engineering area. ICSSE 2023, hosted by Ho Chi Minh City University of Technology and Education (Vietnam), Taiwan Association of System Science and Engineering (TASSE), National Taiwan Ocean University&hellip;",
            "content_html": "<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \"><span style=\"color: #e03e2d;\"><strong><span style=\"color: #2dc26b;\">ICSSE 2023:</span>¬†</strong></span></span>The Annual International Conference on System Science and Engineering, well known as ICSSE, is a reputational conference on System Science and Engineering area.¬† ICSSE 2023, hosted by Ho Chi Minh City University of Technology and Education (Vietnam), Taiwan Association of System Science and Engineering (TASSE), National Taiwan Ocean University (Taiwan) and Rajamangala University of Technology Lanna (Thailand), will take place in Ho Chi Minh City, Vietnam during July 27-28, 2023.¬† The scholars, experts, engineers, and practitioners from all over the world are warmly invited to present the latest enhancements and innovations in the field of system science and engineering, as well as to facilitate interactions between scholars and practitioners. ICSSE 2023 will feature plenary speeches in emerging technology topics given by world renowned scholars. The conference welcomes all submissions of original research papers in all aspects of System Science and Engineering as listed below. The papers will go through a thorough and strict peer-review process in IEEE standards.</p>\n<p dir=\"ltr\">System Science and Engineering is a research area that covers a wide spectrum of modern technologies. A system includes a collection of entities and their interrelationships to jointly form a whole greater than the sum of the entities. It also involves human, organizations, cultures, activities and interrelationships. Nowadays, systems composed of autonomous subsystems are more and more widely used and show the great advantages of expanded data density, connectivity, and ubiquitous computational resources with higher interdependence and interaction complexity. This has in turn made the job of planning, developing and deploying complex systems even more difficult and intelligent.</p>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">CVISLab member presents a contribution on \"</span><span class=\"C9DxTc \">Fusion of ViT Technique and Image Filtering in Deep Learning for Plant Pests and Diseases Recognition</span><span class=\"C9DxTc \">\".</span></p>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">Over all, everything is well organized.</span></p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/12/ICSSE2023_1.JPG\" alt=\"\" width=\"1280\" height=\"798\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_1-xs.JPG 640w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_1-sm.JPG 768w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_1-md.JPG 1024w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_1-lg.JPG 1366w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_1-xl.JPG 1600w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_1-2xl.JPG 1920w\"></figure>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/12/ICSSE2023_2.JPG\" alt=\"\" width=\"960\" height=\"721\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_2-xs.JPG 640w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_2-sm.JPG 768w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_2-md.JPG 1024w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_2-lg.JPG 1366w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_2-xl.JPG 1600w ,https://dunghvcs.github.io/media/posts/12/responsive/ICSSE2023_2-2xl.JPG 1920w\"></figure>",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Conference Events"
            ],
            "date_published": "2026-02-14T09:49:20+07:00",
            "date_modified": "2026-02-14T09:49:20+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/fair-2023.html",
            "url": "https://dunghvcs.github.io/fair-2023.html",
            "title": "FAIR 2023",
            "summary": "FAIR - Fundamental and Applied Information Technology Li√™n hi·ªáp c√°c H·ªôi Khoa h·ªçc v√† K·ªπ thu·∫≠t Vi·ªát Nam, Vi·ªán H√†n l√¢m Khoa h·ªçc v√† C√¥ng ngh·ªá Vi·ªát Nam ph·ªëi h·ª£p c√πng¬†Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng nghi·ªáp H√† N·ªôi¬†v√† c√°c c∆° quan khoa h·ªçc, c√°c nh√† khoa h·ªçc t·ª´ c√°c vi·ªán nghi√™n&hellip;",
            "content_html": "<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span style=\"color: #169179;\"><strong><span class=\"C9DxTc \">FAIR - Fundamental and Applied Information Technology</span><span class=\"C9DxTc \">¬†</span></strong></span></p>\n<p>Li√™n hi·ªáp c√°c H·ªôi Khoa h·ªçc v√† K·ªπ thu·∫≠t Vi·ªát Nam, Vi·ªán H√†n l√¢m Khoa h·ªçc v√† C√¥ng ngh·ªá Vi·ªát Nam ph·ªëi h·ª£p c√πng¬†<span class=\"fontstyle0\">Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng nghi·ªáp H√† N·ªôi¬†</span>v√† c√°c c∆° quan khoa h·ªçc, c√°c nh√† khoa h·ªçc t·ª´ c√°c vi·ªán nghi√™n c·ª©u, c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc ƒë·ªÉ t·ªï ch·ª©c H·ªôi ngh·ªã khoa h·ªçc qu·ªëc gia l·∫ßn th·ª© XVIII v·ªÅ \"Nghi√™n c·ª©u c∆° b·∫£n v√† ·ª©ng d·ª•ng C√¥ng ngh·ªá th√¥ng tin\".</p>\n<p dir=\"ltr\">Ch·ªß ƒë·ªÅ ch√≠nh c·ªßa H·ªôi ngh·ªã l√† \"¬†<span class=\"fontstyle0\">Chuy·ªÉn ƒë·ªïi s·ªë v√† c√°c xu th·∫ø t∆∞∆°ng lai</span>\". Tuy nhi√™n, do truy·ªÅn th·ªëng c·ªßa H·ªôi ngh·ªã n√™n kh√¥ng h·∫°n ch·∫ø v·ªÅ n·ªôi dung. H·ªôi ngh·ªã nƒÉm nay ƒë∆∞·ª£c s·ª± b·∫£o tr·ª£ chuy√™n m√¥n c·ªßa 4 c∆° s·ªü ƒë√†o t·∫°o uy t√≠n trong lƒ©nh v·ª±c c√¥ng ngh·ªá th√¥ng tin ‚Äì truy·ªÅn th√¥ng l√† ƒê·∫°i h·ªçc ƒê√† N·∫µng, H·ªçc vi·ªán C√¥ng ngh·ªá B∆∞u ch√≠nh Vi·ªÖn th√¥ng, Tr∆∞·ªùng ƒê·∫°i hoc C√¥ng ngh·ªá Th√¥ng tin &amp; Truy·ªÅn th√¥ng Th√°i Nguy√™n, Vi·ªán C√¥ng ngh·ªá Th√¥ng tin ‚Äì ƒêHQGHN.<br>FAIR l·∫ßn th·ª© XVIII (FAIR'2025), t·ªï ch·ª©c t·∫°i Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng nghi·ªáp H√† N·ªôi v√†o 2 ng√†y: Th·ª© NƒÉm v√† Th·ª© S√°u, 21 - 22/08/2025.</p>\n<div class=\"gallery-wrapper\"><div class=\"gallery\" data-is-empty=\"false\" data-translation=\"Add images\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://dunghvcs.github.io/media/posts/11/gallery/FAIR2023_1.jpg\" data-size=\"1163x960\"><img loading=\"lazy\" src=\"https://dunghvcs.github.io/media/posts/11/gallery/FAIR2023_1-thumbnail.jpg\" alt=\"\" width=\"720\" height=\"594\"></a></figure>\n<figure class=\"gallery__item\"><a href=\"https://dunghvcs.github.io/media/posts/11/gallery/FAIR2023_2.jpg\" data-size=\"917x515\"><img loading=\"lazy\" src=\"https://dunghvcs.github.io/media/posts/11/gallery/FAIR2023_2-thumbnail.jpg\" alt=\"\" width=\"720\" height=\"404\"></a></figure>\n</div></div>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">CVISLab member presents a contribution on \"Performance Evaluation of Mediapipe and Openpose for Skeleton Data Extraction\"</span></p>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">In this study, our focus lies in evaluating two approaches for extracting skeletal data based on OpenPose and Mediapipe frameworks across the KTH and UTD-MHAD datasets. The skeletal data extracted from both methods serves as input for the AcTv2 model, an enhanced version of the AcT model. Through training and evaluating on the AcTv2 model, we ascertain the effectiveness of both skeletal data extraction methods on specific datasets. The experimental results of this research contribute to a better understanding of the efficacy of these skeletal data extraction methods in providing informative data for the AcTv2 model to recognize human actions across different datasets.</span></p>",
            "image": "https://dunghvcs.github.io/media/posts/11/FAIR2023_2.jpg",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Conference Events"
            ],
            "date_published": "2026-02-14T09:34:04+07:00",
            "date_modified": "2026-02-14T09:34:34+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/digital-image-processing.html",
            "url": "https://dunghvcs.github.io/digital-image-processing.html",
            "title": "Digital Image Processing",
            "summary": "Digital Image Processing: Chapter 1: Introduction to image processing Chapter 2: Image enhancement in space domain Chapter 3: Image enhancement in frequency domain Chapter 4: Morphological image processing Chapter 5: Image segmentation Chapter 6: Feature extraction Chapter 7: Pattern recognition",
            "content_html": "<div class=\"CjVfdc\">Digital Image Processing:<br>\n<p>Chapter 1: Introduction to image processing</p>\n<p>Chapter 2: Image enhancement in space domain</p>\n<p>Chapter 3: Image enhancement in frequency domain</p>\n<p>Chapter 4: Morphological image processing</p>\n<p>Chapter 5: Image segmentation</p>\n<p>Chapter 6: Feature extraction</p>\n<p>Chapter 7: Pattern recognition</p>\n</div>\n<div><figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/8/DIP1-2.png\" alt=\"\" width=\"813\" height=\"594\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/8/responsive/DIP1-2-xs.png 640w ,https://dunghvcs.github.io/media/posts/8/responsive/DIP1-2-sm.png 768w ,https://dunghvcs.github.io/media/posts/8/responsive/DIP1-2-md.png 1024w ,https://dunghvcs.github.io/media/posts/8/responsive/DIP1-2-lg.png 1366w ,https://dunghvcs.github.io/media/posts/8/responsive/DIP1-2-xl.png 1600w ,https://dunghvcs.github.io/media/posts/8/responsive/DIP1-2-2xl.png 1920w\"></figure></div>\n<div>\n<p>¬†</p>\n</div>\n<h3 id=\"h.i1ci6wnis21l_l\" class=\"CDt4Ke zfr3Q OmQG5e\" dir=\"ltr\" tabindex=\"-1\"></h3>",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Courses"
            ],
            "date_published": "2026-02-13T11:56:48+07:00",
            "date_modified": "2026-02-13T11:56:48+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/year-end-seminar.html",
            "url": "https://dunghvcs.github.io/year-end-seminar.html",
            "title": "Year end seminar¬†",
            "summary": "As we close the chapter on 2025, it is time to look back at a year filled with rigorous research, collaborative efforts, and meaningful contributions to the scientific community. This year-end seminar marks not just the passing of time, but the accumulation of knowledge and the expansion of our research&hellip;",
            "content_html": "<p>As we close the chapter on 2025, it is time to look back at a year filled with rigorous research, collaborative efforts, and meaningful contributions to the scientific community. This year-end seminar marks not just the passing of time, but the accumulation of knowledge and the expansion of our research capabilities in <strong data-path-to-node=\"4\" data-index-in-node=\"331\">Artificial Intelligence, Computer Vision, and Educational Technology</strong>.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/7/2025Khenthuong.jpg\" alt=\"\" width=\"2048\" height=\"1365\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/7/responsive/2025Khenthuong-xs.jpg 640w ,https://dunghvcs.github.io/media/posts/7/responsive/2025Khenthuong-sm.jpg 768w ,https://dunghvcs.github.io/media/posts/7/responsive/2025Khenthuong-md.jpg 1024w ,https://dunghvcs.github.io/media/posts/7/responsive/2025Khenthuong-lg.jpg 1366w ,https://dunghvcs.github.io/media/posts/7/responsive/2025Khenthuong-xl.jpg 1600w ,https://dunghvcs.github.io/media/posts/7/responsive/2025Khenthuong-2xl.jpg 1920w\"></figure>\n<p data-path-to-node=\"6\"><strong data-path-to-node=\"6\" data-index-in-node=\"0\">üìä Research Projects &amp; Grants</strong> This year, our lab continued to push the boundaries of applied AI. We successfully maintained momentum on key projects focusing on:</p>\n<ul data-path-to-node=\"7\">\n<li>\n<p data-path-to-node=\"7,0,0\"><strong data-path-to-node=\"7,0,0\" data-index-in-node=\"0\">AI in Healthcare:</strong> Developing advanced diagnostic tools for medical imaging.</p>\n</li>\n<li>\n<p data-path-to-node=\"7,1,0\"><strong data-path-to-node=\"7,1,0\" data-index-in-node=\"0\">Intelligent Systems:</strong> Enhancing autonomous recognition capabilities for aerial and surveillance applications.</p>\n</li>\n<li>\n<p data-path-to-node=\"7,2,0\"><strong data-path-to-node=\"7,2,0\" data-index-in-node=\"0\">AI in Education:</strong> Applying deep learning to personalize and improve e-learning environments.</p>\n</li>\n</ul>\n<p data-path-to-node=\"8\"><i data-path-to-node=\"8\" data-index-in-node=\"0\">(Note: You can insert the exact number of funded projects here, e.g., \"We successfully executed <strong data-path-to-node=\"8\" data-index-in-node=\"96\">3</strong> major research grants...\")</i></p>\n<p data-path-to-node=\"9\"><strong data-path-to-node=\"9\" data-index-in-node=\"0\">üìù Key Publications (Journals &amp; Conferences)</strong> 2025 was a prolific year for dissemination. We are proud to have published impactful papers in reputable venues, addressing both theoretical improvements and practical applications of Deep Learning.</p>\n<p data-path-to-node=\"9\">ü§ù Community Service &amp; Leadership</p>\n<p data-path-to-node=\"9\">These achievements would not have been possible without the dedication of our research team, graduate students, and collaborators. As we move into <strong data-path-to-node=\"14\" data-index-in-node=\"157\">2026</strong>, we remain committed to solving real-world problems through innovation in Artificial Intelligence.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/7/YearEndParty.jpg\" alt=\"\" width=\"2560\" height=\"1440\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/7/responsive/YearEndParty-xs.jpg 640w ,https://dunghvcs.github.io/media/posts/7/responsive/YearEndParty-sm.jpg 768w ,https://dunghvcs.github.io/media/posts/7/responsive/YearEndParty-md.jpg 1024w ,https://dunghvcs.github.io/media/posts/7/responsive/YearEndParty-lg.jpg 1366w ,https://dunghvcs.github.io/media/posts/7/responsive/YearEndParty-xl.jpg 1600w ,https://dunghvcs.github.io/media/posts/7/responsive/YearEndParty-2xl.jpg 1920w\"></figure>",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Seminars"
            ],
            "date_published": "2026-02-13T11:54:05+07:00",
            "date_modified": "2026-02-13T11:54:05+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/development-of-an-intelligent-system-for-ambient-understanding-applied-to-autonomous-navigation-in-outdoor-environment.html",
            "url": "https://dunghvcs.github.io/development-of-an-intelligent-system-for-ambient-understanding-applied-to-autonomous-navigation-in-outdoor-environment.html",
            "title": "[NAFOSTED] Project ambient understanding",
            "summary": "<p><span style=\"color: #169179;\"><strong><span style=\"color: #169179;\">Development of an intelligent system for ambient understanding applied to autonomous navigation in outdoor environment.</span>¬†¬†</strong></span></p>\n<p>This project proposes and studies a novel method that improves efficiency of scene understanding, which is useful for autonomous navigation, based on the multiple sensors-based. The method is expected to provide the better scene understanding, which then allows a robot to avoid obstacles in front of its navigation. The method consists of some stages. First, a path planning is investigated, which concerns about the findings of the efficient path to facilitate the autonomous driving. The second is referred to the problem of the motion estimation and the localization prediction of running vehicle. It is addressed based on applying the fusion of cameras, LRF and GPS devices. Our research proposes a new method that uses the minimal set of parameters consisting of the geometrical constraints for estimating the 3D motion of vehicle using cameras and LRF. The cumulative errors of visual odometry are excluded using the GPS-based correction relied on the maximum likelihood estimation in particle filter. The semantic of object classification and detection are also studied. The obstacle detection, place recognition techniques are used as a solution to assisting the autonomous driver. We focus on dealing with detecting obstacles that commonly occur such as pedestrians and vehicles. The place recognition is also considered for scene understanding and mapping. We introduce improving of feature descriptors, deformable part model as well hybrid boosting SVM and neural network.¬†</p>\n",
            "content_html": "<p><span style=\"color: #169179;\"><strong><span style=\"color: #169179;\">Development of an intelligent system for ambient understanding applied to autonomous navigation in outdoor environment.</span>¬†¬†</strong></span></p>\n<p>This project proposes and studies a novel method that improves efficiency of scene understanding, which is useful for autonomous navigation, based on the multiple sensors-based. The method is expected to provide the better scene understanding, which then allows a robot to avoid obstacles in front of its navigation. The method consists of some stages. First, a path planning is investigated, which concerns about the findings of the efficient path to facilitate the autonomous driving. The second is referred to the problem of the motion estimation and the localization prediction of running vehicle. It is addressed based on applying the fusion of cameras, LRF and GPS devices. Our research proposes a new method that uses the minimal set of parameters consisting of the geometrical constraints for estimating the 3D motion of vehicle using cameras and LRF. The cumulative errors of visual odometry are excluded using the GPS-based correction relied on the maximum likelihood estimation in particle filter. The semantic of object classification and detection are also studied. The obstacle detection, place recognition techniques are used as a solution to assisting the autonomous driver. We focus on dealing with detecting obstacles that commonly occur such as pedestrians and vehicles. The place recognition is also considered for scene understanding and mapping. We introduce improving of feature descriptors, deformable part model as well hybrid boosting SVM and neural network.¬†</p>\n\n<div class=\"gallery-wrapper\"><div class=\"gallery\" data-is-empty=\"false\" data-translation=\"Add images\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://dunghvcs.github.io/media/posts/5//gallery/NF1.png\" data-size=\"956x916\"><img loading=\"lazy\" src=\"https://dunghvcs.github.io/media/posts/5//gallery/NF1-thumbnail.png\" alt=\"\" width=\"720\" height=\"690\"></a></figure>\n<figure class=\"gallery__item\"><a href=\"https://dunghvcs.github.io/media/posts/5//gallery/NF2.png\" data-size=\"677x540\"><img loading=\"lazy\" src=\"https://dunghvcs.github.io/media/posts/5//gallery/NF2-thumbnail.png\" alt=\"\" width=\"677\" height=\"540\"></a></figure>\n</div></div>",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Researches"
            ],
            "date_published": "2026-02-13T10:18:33+07:00",
            "date_modified": "2026-02-13T22:48:28+07:00"
        },
        {
            "id": "https://dunghvcs.github.io/aciids2017-kanazawa-japan.html",
            "url": "https://dunghvcs.github.io/aciids2017-kanazawa-japan.html",
            "title": "ACIIDS2017, Kanazawa, Japan",
            "summary": "ACIIDS is an international conference for researches in the filed of intelligent information and database systems. In this year, this conference was held during 3-5 April 2017 in Kaga, Japan. The aim of the conference is to provide an internationally respected forum for scientific research in the technologies and applications&hellip;",
            "content_html": "<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">ACIIDS is an international conference for researches in the filed of intelligent information and database systems. In this year, this conference was held during 3-5 April 2017 in Kaga, Japan. The aim of the conference is to provide an internationally respected forum for scientific research in the technologies and applications of intelligent information and database systems.</span></p>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">Participant from ISLab is Dr. Van-Dung Hoang with a contribution \"Boosting Discriminative Models for Activity Detection Using Local Feature Descriptors\"</span></p>\n<p class=\"zfr3Q CDt4Ke \" dir=\"ltr\"><span class=\"C9DxTc \">Over all, everything is well organized.</span></p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://dunghvcs.github.io/media/posts/4/ACIIDS2017.jpg\" alt=\"\" width=\"1177\" height=\"883\" sizes=\"(max-width: 1920px) 100vw, 1920px\" srcset=\"https://dunghvcs.github.io/media/posts/4/responsive/ACIIDS2017-xs.jpg 640w ,https://dunghvcs.github.io/media/posts/4/responsive/ACIIDS2017-sm.jpg 768w ,https://dunghvcs.github.io/media/posts/4/responsive/ACIIDS2017-md.jpg 1024w ,https://dunghvcs.github.io/media/posts/4/responsive/ACIIDS2017-lg.jpg 1366w ,https://dunghvcs.github.io/media/posts/4/responsive/ACIIDS2017-xl.jpg 1600w ,https://dunghvcs.github.io/media/posts/4/responsive/ACIIDS2017-2xl.jpg 1920w\"></figure>",
            "author": {
                "name": "Van-Dung Hoang"
            },
            "tags": [
                   "Conference Events"
            ],
            "date_published": "2026-02-13T09:29:56+07:00",
            "date_modified": "2026-02-14T09:26:29+07:00"
        }
    ]
}
