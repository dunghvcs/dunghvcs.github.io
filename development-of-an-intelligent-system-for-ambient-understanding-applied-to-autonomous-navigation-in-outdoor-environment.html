<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>[NAFOSTED] Project ambient understanding - dunghv.github.io</title><meta name="description" content="Development of an intelligent system for ambient understanding applied to autonomous navigation in outdoor environment.  This project proposes and studies a novel method that improves efficiency of scene understanding, which is useful for autonomous navigation, based on the multiple sensors-based. The method is expected to provide the better scene understanding, which then allows a robot to avoid obstacles in front of its navigation. The method consists of some stages. First, a path planning is investigated, which concerns about the findings of the efficient path to facilitate the autonomous driving. The second is referred to the problem of the motion estimation and the localization prediction of running vehicle. It is addressed based on applying the fusion of cameras, LRF and GPS devices. Our research proposes a new method that uses the minimal set of parameters consisting of the geometrical constraints for estimating the 3D motion of vehicle using cameras and LRF. The cumulative errors of visual odometry are excluded using the GPS-based correction relied on the maximum likelihood estimation in particle filter. The semantic of object classification and detection are also studied. The obstacle detection, place recognition techniques are used as a solution to assisting the autonomous driver. We focus on dealing with detecting obstacles that commonly occur such as pedestrians and vehicles. The place recognition is also considered for scene understanding and mapping. We introduce improving of feature descriptors, deformable part model as well hybrid boosting SVM and neural network. "><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://dunghvcs.github.io/development-of-an-intelligent-system-for-ambient-understanding-applied-to-autonomous-navigation-in-outdoor-environment.html"><link rel="alternate" type="application/atom+xml" href="https://dunghvcs.github.io/feed.xml" title="dunghv.github.io - RSS"><link rel="alternate" type="application/json" href="https://dunghvcs.github.io/feed.json" title="dunghv.github.io - JSON"><meta property="og:title" content="[NAFOSTED] Project ambient understanding - dunghv.github.io "><meta property="og:image" content="https://dunghvcs.github.io/media/website/tinh1.jpg"><meta property="og:image:width" content="1280"><meta property="og:image:height" content="586"><meta property="og:site_name" content="dunghv.github.io"><meta property="og:description" content="Development of an intelligent system for ambient understanding applied to autonomous navigation in outdoor environment.  This project proposes and studies a novel method that improves efficiency of scene understanding, which is useful for autonomous navigation, based on the multiple sensors-based. The method is expected to provide the better scene understanding, which then allows a robot to avoid obstacles in front of its navigation. The method consists of some stages. First, a path planning is investigated, which concerns about the findings of the efficient path to facilitate the autonomous driving. The second is referred to the problem of the motion estimation and the localization prediction of running vehicle. It is addressed based on applying the fusion of cameras, LRF and GPS devices. Our research proposes a new method that uses the minimal set of parameters consisting of the geometrical constraints for estimating the 3D motion of vehicle using cameras and LRF. The cumulative errors of visual odometry are excluded using the GPS-based correction relied on the maximum likelihood estimation in particle filter. The semantic of object classification and detection are also studied. The obstacle detection, place recognition techniques are used as a solution to assisting the autonomous driver. We focus on dealing with detecting obstacles that commonly occur such as pedestrians and vehicles. The place recognition is also considered for scene understanding and mapping. We introduce improving of feature descriptors, deformable part model as well hybrid boosting SVM and neural network. "><meta property="og:url" content="https://dunghvcs.github.io/development-of-an-intelligent-system-for-ambient-understanding-applied-to-autonomous-navigation-in-outdoor-environment.html"><meta property="og:type" content="article"><link rel="shortcut icon" href="https://dunghvcs.github.io/media/website/CVIS.png" type="image/x-icon"><link rel="stylesheet" href="https://dunghvcs.github.io/assets/css/style.css?v=3335bf37d3b19114f081ffcb0714ad69"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://dunghvcs.github.io/development-of-an-intelligent-system-for-ambient-understanding-applied-to-autonomous-navigation-in-outdoor-environment.html"},"headline":"[NAFOSTED] Project ambient understanding","datePublished":"2026-02-13T10:18+07:00","dateModified":"2026-02-13T22:48+07:00","image":{"@type":"ImageObject","url":"https://dunghvcs.github.io/media/website/CVISLab-removebg-preview.png","height":311,"width":741},"description":"Development of an intelligent system for ambient understanding applied to autonomous navigation in outdoor environment.  This project proposes and studies a novel method that improves efficiency of scene understanding, which is useful for autonomous navigation, based on the multiple sensors-based. The method is expected to provide the better scene understanding, which then allows a robot to avoid obstacles in front of its navigation. The method consists of some stages. First, a path planning is investigated, which concerns about the findings of the efficient path to facilitate the autonomous driving. The second is referred to the problem of the motion estimation and the localization prediction of running vehicle. It is addressed based on applying the fusion of cameras, LRF and GPS devices. Our research proposes a new method that uses the minimal set of parameters consisting of the geometrical constraints for estimating the 3D motion of vehicle using cameras and LRF. The cumulative errors of visual odometry are excluded using the GPS-based correction relied on the maximum likelihood estimation in particle filter. The semantic of object classification and detection are also studied. The obstacle detection, place recognition techniques are used as a solution to assisting the autonomous driver. We focus on dealing with detecting obstacles that commonly occur such as pedestrians and vehicles. The place recognition is also considered for scene understanding and mapping. We introduce improving of feature descriptors, deformable part model as well hybrid boosting SVM and neural network. ","author":{"@type":"Person","name":"Van-Dung Hoang","url":"https://dunghvcs.github.io/authors/van-dung-hoang/"},"publisher":{"@type":"Organization","name":"Van-Dung Hoang","logo":{"@type":"ImageObject","url":"https://dunghvcs.github.io/media/website/CVISLab-removebg-preview.png","height":311,"width":741}}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript><meta name="google-site-verification" content="M_Rg_xJ87RXCGmMfdbThkCZq_Vf2d2dgySWK4LY3BB8"><style>/* Cấu hình khung hiển thị slider */
  .my-slider-container {
    width: 100%; /* Chiều rộng luôn là 100% */
    position: relative;
    overflow: hidden;
    margin: 0;
    padding: 0;
  }

  /* Phần chứa các ảnh */
  .my-slides {
    display: flex;
    transition: transform 0.5s ease-in-out; /* Hiệu ứng trượt mượt */
    width: 100%;
  }

  /* Cấu hình từng ảnh */
  .my-slides img {
    width: 100%; /* Ảnh luôn rộng 100% màn hình */
    object-fit: cover; /* Cắt ảnh tự động để lấp đầy khung mà không bị méo */
    display: block;
    flex-shrink: 0;
  }

  /* --- QUAN TRỌNG: Điều chỉnh chiều cao theo màn hình --- */
  
  /* Mặc định cho Máy tính (PC/Laptop) */
  .my-slides img {
    height: 300px; /* Chiều cao banner trên máy tính */
  }

  /* Dành cho Máy tính bảng (Tablet) */
  @media (max-width: 1024px) {
    .my-slides img {
      height: 450px; 
    }
  }

  /* Dành cho Điện thoại (Mobile) */
  @media (max-width: 768px) {
    .my-slides img {
      height: 250px; /* Chiều cao nhỏ hơn để vừa màn hình điện thoại */
    }
  }

  /* Nút mũi tên Trái/Phải */
  .prev, .next {
    cursor: pointer;
    position: absolute;
    top: 50%;
    transform: translateY(-50%); /* Căn giữa theo chiều dọc chuẩn xác hơn */
    padding: 16px;
    color: white;
    font-weight: bold;
    font-size: 24px;
    transition: 0.3s ease;
    user-select: none;
    background-color: rgba(0,0,0,0.2); /* Nền mờ nhẹ */
    border: none;
    z-index: 2;
  }

  /* Vị trí nút */
  .prev { left: 0; border-radius: 0 5px 5px 0; }
  .next { right: 0; border-radius: 5px 0 0 5px; }

  /* Hiệu ứng khi di chuột vào nút */
  .prev:hover, .next:hover {
    background-color: rgba(0,0,0,0.8);
  }</style></head><body class="post-template"><nav class="my-custom-navbar"><div class="nav-logo"><a href="/"><img src="https://dunghvcs.github.io/media/files/CVISLab_nbg.png?text=CVIS" alt="Logo"> <span>Van-Dung Hoang</span></a></div><button class="menu-toggle" onclick="toggleMobileMenu()">&#9776;</button><ul class="nav-links" id="myMobileNav"><li><a href="/">Home</a></li><li class="dropdown"><a href="/tags/news">News ▼</a><ul class="dropdown-menu"><li><a href="/tags/newsconf">Conferences</a></li><li><a href="/tags/seminar">Seminars</a></li><li><a href="/tags/news">Other events</a></li></ul></li><li><a href="/tags/courses">Courses</a></li><li><a href="/tags/research">Research</a></li><li><a href="/publications.html">Publications</a></li><li><a href="/contact.html">Contact</a></li><li><a href="/about.html">About</a></li></ul></nav><script>function toggleMobileMenu() {
    var nav = document.getElementById("myMobileNav");
    // Kiểm tra nếu đang hiện thì ẩn, đang ẩn thì hiện
    if (nav.classList.contains("active")) {
      nav.classList.remove("active");
    } else {
      nav.classList.add("active");
    }
  }</script><div class="my-slider-container"><div class="my-slides"><img src="https://images.unsplash.com/photo-1493246507139-91e8fad9978e?ixlib=rb-4.0.3&auto=format&fit=crop&w=1600&q=80" alt="Ảnh 1"> <img src="https://images.unsplash.com/photo-1518173946687-a4c8892bbd9f?ixlib=rb-4.0.3&auto=format&fit=crop&w=1600&q=80" alt="Ảnh 2"> <img src="https://images.unsplash.com/photo-1506744038136-46273834b3fb?ixlib=rb-4.0.3&auto=format&fit=crop&w=1600&q=80" alt="Ảnh 3"> <img src="https://images.unsplash.com/photo-1501785888041-af3ef285b470?ixlib=rb-4.0.3&auto=format&fit=crop&w=1600&q=80" alt="Ảnh 4"> <img src="https://images.unsplash.com/photo-1470770841072-f978cf4d019e?ixlib=rb-4.0.3&auto=format&fit=crop&w=1600&q=80" alt="Ảnh 5"></div><button class="prev" onclick="moveSlide(-1)">&#10094;</button> <button class="next" onclick="moveSlide(1)">&#10095;</button></div><script>let slideIndex = 0;
  
  // Hàm chuyển slide
  function moveSlide(n) {
    const slides = document.querySelector('.my-slides');
    const images = document.querySelectorAll('.my-slides img');
    const totalSlides = images.length;

    slideIndex += n;
    
    // Xử lý vòng lặp (về đầu hoặc về cuối)
    if (slideIndex >= totalSlides) {
      slideIndex = 0;
    }
    if (slideIndex < 0) {
      slideIndex = totalSlides - 1;
    }
    
    // Trượt theo % (Mỗi ảnh chiếm 100% chiều rộng)
    slides.style.transform = `translateX(-${slideIndex * 100}%)`;
  }

  // Tự động chạy sau mỗi 5 giây
  setInterval(() => {
    moveSlide(1);
  }, 5000);</script><header class="top js-header"><a class="logo" href="https://dunghvcs.github.io/"><img src="https://dunghvcs.github.io/media/website/CVISLab-removebg-preview.png" alt="dunghv.github.io" width="741" height="311"></a></header><main class="post"><article class="content"><div class="hero hero--noimage"><header class="hero__content"><div class="wrapper"><h1>[NAFOSTED] Project ambient understanding</h1></div></header></div><div class="entry-wrapper content__entry"><p><span style="color: #169179;"><strong><span style="color: #169179;">Development of an intelligent system for ambient understanding applied to autonomous navigation in outdoor environment.</span>  </strong></span></p><p>This project proposes and studies a novel method that improves efficiency of scene understanding, which is useful for autonomous navigation, based on the multiple sensors-based. The method is expected to provide the better scene understanding, which then allows a robot to avoid obstacles in front of its navigation. The method consists of some stages. First, a path planning is investigated, which concerns about the findings of the efficient path to facilitate the autonomous driving. The second is referred to the problem of the motion estimation and the localization prediction of running vehicle. It is addressed based on applying the fusion of cameras, LRF and GPS devices. Our research proposes a new method that uses the minimal set of parameters consisting of the geometrical constraints for estimating the 3D motion of vehicle using cameras and LRF. The cumulative errors of visual odometry are excluded using the GPS-based correction relied on the maximum likelihood estimation in particle filter. The semantic of object classification and detection are also studied. The obstacle detection, place recognition techniques are used as a solution to assisting the autonomous driver. We focus on dealing with detecting obstacles that commonly occur such as pedestrians and vehicles. The place recognition is also considered for scene understanding and mapping. We introduce improving of feature descriptors, deformable part model as well hybrid boosting SVM and neural network. </p><div class="gallery-wrapper"><div class="gallery" data-is-empty="false" data-translation="Add images" data-columns="3"><figure class="gallery__item"><a href="https://dunghvcs.github.io/media/posts/5//gallery/NF1.png" data-size="956x916"><img loading="lazy" src="https://dunghvcs.github.io/media/posts/5//gallery/NF1-thumbnail.png" alt="" width="720" height="690"></a></figure><figure class="gallery__item"><a href="https://dunghvcs.github.io/media/posts/5//gallery/NF2.png" data-size="677x540"><img loading="lazy" src="https://dunghvcs.github.io/media/posts/5//gallery/NF2-thumbnail.png" alt="" width="677" height="540"></a></figure></div></div></div></article></main><footer class="footer"><div class="wrapper"><nav class="footer__nav"><ul><li><a href="https://dunghvcs.github.io/" class="al" target="_self">Home</a></li><li><a href="https://dunghvcs.github.io/tags/newsconf/" class="al" target="_self">News</a></li><li><a href="https://dunghvcs.github.io/tags/courses/" class="al" target="_self">Courses</a></li><li><a href="https://dunghvcs.github.io/tags/research/" class="al" target="_self">Research</a></li><li><a href="https://dunghvcs.github.io/publications.html" class="al" target="_self">Publications</a></li><li><a href="https://dunghvcs.github.io/" class="al" target="_self">Dataset</a></li><li><a href="https://dunghvcs.github.io/contact.html" class="al" target="_self">Contact</a></li><li><a href="https://dunghvcs.github.io/about.html" class="al" target="_self">About</a></li></ul></nav><div class="footer__copyright"><p>Copyright@CVISLab</p></div><button id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="https://dunghvcs.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="https://dunghvcs.github.io/assets/js/scripts.min.js?v=d0fc1030089a37f93a2d51bf6d07565c"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 0,doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>